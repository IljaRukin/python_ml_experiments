{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "## discrete\n",
    "probality to measure x<br>\n",
    "\\begin{equation}\n",
    "p(x)\n",
    "\\end{equation}\n",
    "\n",
    "### Mean\n",
    "\\begin{equation}\n",
    "\\mu (x) = \\sum_i x_i \\cdot p(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "### Variance\n",
    "\\begin{equation}\n",
    "var (x) = \\mu \\left( (x - \\mu(x))^2 \\right)\\\\\n",
    "= \\sum_i (x_i - \\mu(x))^2 \\cdot p(x_i)\n",
    "\\end{equation}\n",
    "\n",
    "### Standard deviation\n",
    "\\begin{equation}\n",
    "\\sigma = \\sqrt{ var(x) }\n",
    "\\end{equation}\n",
    "\n",
    "## continuous\n",
    "\n",
    "### Probability density function\n",
    "\\begin{equation}\n",
    "f(x)\n",
    "\\end{equation}\n",
    "\n",
    "### Cumulative distribution function\n",
    "probability to measure x between a and b<br>\n",
    "\\begin{equation}\n",
    "P(a \\leq x \\leq b) = \\int_a^b f(x) dx\n",
    "\\end{equation}\n",
    "\n",
    "### Mean\n",
    "\\begin{equation}\n",
    "\\mu (x) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) dx\n",
    "\\end{equation}\n",
    "\n",
    "### Variance\n",
    "\\begin{equation}\n",
    "var (x) = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 \\cdot f(x) dx\n",
    "\\end{equation}\n",
    "\n",
    "### Standard deviation\n",
    "\\begin{equation}\n",
    "\\sigma = \\sqrt{ var(x) }\n",
    "\\end{equation}\n",
    "\n",
    "## moments\n",
    "\n",
    "- median: value from wich left and right lie 50% of the distribution\n",
    "- variance: dispersion of the distribution\n",
    "- symmetry: amount how mirror-symmetric the distribution is\n",
    "- skewness: amount the distribution leans to one side\n",
    "- kurtosis: fatness of the tails of the distribution\n",
    "\n",
    "## important distributions\n",
    "\n",
    "### Bernoulli Distribution\n",
    "probability for a random experiment with two possible outcomes.<br>\n",
    "for example: x=0 failure, x=1 success;<br>\n",
    "\\begin{equation}\n",
    "f(x \\in \\lbrack 0,1 \\rbrack) =\n",
    "\\begin{cases}\n",
    "p& \\text{if $x=1$}\\\\\n",
    "1-p& \\text{if $x=0$}\n",
    "\\end{cases}\\\\\n",
    "= p^x(1-p)^{1-x}\n",
    "\\end{equation}\n",
    "\n",
    "### Geometric Distribution\n",
    "if this experiment is performed n times, the probability for $x$ successful attempts in a particular order is<br>\n",
    "\\begin{equation}\n",
    "f(x) = p^x(1-p)^{n-x}\n",
    "\\end{equation}\n",
    "\n",
    "### Binomial Distribution\n",
    "if the order the events are occuring is not important, the probability for $x$ becomes\n",
    "\\begin{equation}\n",
    "f(x) = \\begin{pmatrix} n\\\\x \\end{pmatrix} p^x(1-p)^{n-x}\n",
    "\\end{equation}\n",
    "with $\\begin{pmatrix} n\\\\x \\end{pmatrix} = \\frac{n!}{x!(n-x)!}$, wich is composed of $\\frac{n!}{(n-x)!}$ possible arrangements of x successful outcomes in n trials divided by $x!$ possible permutations of these x successful outcomes (if any two successes were switched place, the outcome would be the same!).<br><br>\n",
    "\n",
    "Example: 5 coin tosses (3 heads (0) & 2 tails (1))<br><br>\n",
    "N=5; x=2;\n",
    "\n",
    "\\begin{align}\n",
    "\\text{possible arrangements of k:} &\\frac{n!}{(n-x)!} = 5 \\cdot 4 \\cdot 3 \\\\\n",
    "\\text{possible permutations of k:} &\\frac{1}{x!} = \\frac{1}{3 \\cdot 2 \\cdot 1} \\\\\n",
    "\\text{possible effective arrangements of k:} &\\frac{n!}{(n-x)! \\cdot x!} =10\n",
    "\\end{align}\n",
    "\n",
    "possible effective (different) coin toss arrangement: <br><br>\n",
    "00111 <br>\n",
    "01011 <br>\n",
    "01101 <br>\n",
    "01110 <br>\n",
    "10011 <br>\n",
    "10101 <br>\n",
    "10110 <br>\n",
    "11001 <br>\n",
    "11010 <br>\n",
    "11100 <br>\n",
    "\n",
    "### Poisson Distributions\n",
    "for small probabilities p and big sample size n the binomial distribution can be approximated by a continuous poisson distribution<br>\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{\\mu^x}{x!} e^{-\\mu}\n",
    "\\end{equation}\n",
    "\n",
    "### Gauss/Normal-Distribution\n",
    "additionally, if $x$ or $(n-x)$ is sufficiently big, the distribution get's symmetrical and can be approximated by a Normal distribution\n",
    "\\begin{equation}\n",
    "f(x) = \\frac{1}{\\sigma \\sqrt{2 \\pi}} e^{-\\frac{(x-\\mu)^2}{2 \\sigma^2}}\n",
    "\\end{equation}\n",
    "\n",
    "### negative Binomial Distribution\n",
    "the probability to get a x-1 successes with n-1 trials and an additional x-th success at trial number n afterwards <br>\n",
    "\\begin{equation}\n",
    "f(x) = \\begin{pmatrix} n-1\\\\x-1 \\end{pmatrix} p^{x-1}(1-p)^{n-1-(x-1)} \\cdot p\\\\\n",
    "= \\begin{pmatrix} n-1\\\\x-1 \\end{pmatrix} p^{x}(1-p)^{n-x}\n",
    "\\end{equation}\n",
    "\n",
    "<br> another formulation is for getting an additional success after $b=n-x$ errors with n trials<br>\n",
    "\\begin{equation}\n",
    "p(x) = \\begin{pmatrix} b+x-1\\\\x-1 \\end{pmatrix} p^{x}(1-p)^{b}\\\\\n",
    "= \\begin{pmatrix} b+x-1\\\\b \\end{pmatrix} p^{x}(1-p)^{b}\n",
    "\\end{equation}the second equation is equal to the first because the binomial coefficient is symmetrical around n/2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binomial Distribution: probability in Lotto to pick 6 numbers out of 49 is 1 in 13983816\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def nCr(n,k):\n",
    "    return np.math.factorial(n)/(np.math.factorial(n-k)*np.math.factorial(k))\n",
    "print('Binomial Distribution: probability in Lotto to pick 6 numbers out of 49 is 1 in',int(nCr(49,6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "The probability density function tells us how likely an event x is to occur. For example a coin toss n times has following probability for x successes\n",
    "\n",
    "\\begin{equation}\n",
    "f(x|p) = p^x(1-p)^{n-x}\n",
    "\\end{equation}\n",
    "\n",
    "where the probability $p$ for a success or $(p-1)$ for a failure is known prior. If we were to perform a coin toss experiment a couple of times, we would get each time the number of successes x, but we neither know the probability of a single coin toss p nor the combined probability $f$.\n",
    "The solution is to guess $f(p|x)$ (values & distribution function) after many tosses and determine the probability of a single coin toss p from the likelihood function.\n",
    "\n",
    "\\begin{equation}\n",
    "f(p|x) = p^x(1-p)^{n-x}\n",
    "\\end{equation}\n",
    "\n",
    "The important difference between a probability and a likelihood is, that the probability $f(x|p)$ returns the probability $f$ of $x$ happening if the parameter $p$ is known. In contrast the likelihood $f(p|x)$ returns the probability $f$ if the parameter $p$ had a particular value when the event $x$ occured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## maximum likelihood estimation\n",
    "\n",
    "The likelihood of events $x_1,...,x_n$ occuring from a probability distribution P with the parameters $\\Theta$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\Theta|X) = P(\\Theta|x_1) \\cdot P(\\Theta|x_2) \\cdot ... \\cdot P(\\Theta|x_n)\n",
    "\\end{equation}\n",
    "\n",
    "To calculate the maximum Likelhood we take the derivative equal to zero. But befor that we calculate the log likelihood to make the calculation easier (products become additions).\n",
    "\n",
    "\\begin{equation}\n",
    "\\log P(\\Theta|X) = \\log P(\\Theta|x_1) + \\log P(\\Theta|x_2) + ... + \\log P(\\Theta|x_n)\n",
    "\\end{equation}\n",
    "\n",
    "Now taking the derivative equal to zero in respect to the parameters $\\Theta$ we get the optimal Parameters one at a time\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial \\Theta_i} \\log P(\\Theta|X) = 0\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example maximum likelihood\n",
    "\n",
    "For example We have weight 5 marbles with weights 5.0g,4.8g,5.1g,4.0g,4.9g and assume a normal distribution of weights.\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\mu,\\sigma|x_i) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\bigg(\\frac{x_i-\\mu}{\\sigma}\\bigg)^2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "P(\\mu,\\sigma|X) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\bigg(\\frac{x_i-\\mu}{\\sigma}\\bigg)^2}\n",
    "\\end{equation}\n",
    "\n",
    "log likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "\\log P(\\mu,\\sigma|X) = \\sum_{i=1}^n -\\frac{1}{2} \\bigg( \\log \\big( 2\\pi \\big) + 2 \\log \\big( \\sigma \\big) + \\Big(\\frac{x_i-\\mu}{\\sigma}\\Big)^2 \\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "derivates\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\mu} \\log P(\\mu,\\sigma|X) &= \\sum_{i=1}^n \\frac{x_i-\\mu}{\\sigma^2} \\\\\n",
    "&= \\frac{1}{\\sigma^2} \\big( \\Big( \\sum_{i=1}^n x_i \\Big) -n\\mu \\big)\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\sigma} \\log P(\\mu,\\sigma|X) &= \\Big( \\sum_{i=1}^n -\\frac{1}{\\sigma} + \\frac{(x_i-\\mu)^2}{\\sigma^3} \\Big) \\\\\n",
    "&= \\frac{1}{\\sigma^3} \\Big( \\sum_{i=1}^n \\big( x_i -\\mu \\big)^2 \\Big) - \\frac{n}{\\sigma}\n",
    "\\end{align}\n",
    "\n",
    "setting equal to zero\n",
    "\n",
    "\\begin{align}\n",
    "\\mu = \\frac{\\sum_{i=1}^n x_i}{n}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\sigma^2 = \\frac{\\sum_{i=1}^n \\big( x_i -\\mu \\big)^2}{n}\n",
    "\\end{align}\n",
    "\n",
    "we get the parameters for maximum likelihood, wich we can input in the normal distribution to get our final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "\\begin{equation}\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "\\end{equation}\n",
    "\n",
    "## Bayes Theorem example\n",
    "\n",
    "56 cards deck, 26 black/ 26 red\n",
    "\n",
    "probability that a red card is a 4\n",
    "\n",
    "\\begin{equation}\n",
    "P(4|red) = \\frac{P(red|4) \\cdot P(4)}{P(red)} = \\frac{1/2 \\cdot 1/13}{1/2} = 1/13\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error for binary predictions\n",
    "\n",
    "\n",
    "Data example:\n",
    "\n",
    "<style>\n",
    "table, th, td {\n",
    "  padding: 5px;\n",
    "  text-align: center;\n",
    "  border: 1px solid black;\n",
    "  border-collapse: collapse;\n",
    "}\n",
    "p{\n",
    "  margin:0px;\n",
    "  font-size:16px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "  \t<td rowspan=\"2\" colspan=\"2\" ></td>\n",
    "  \t<td colspan=\"2\" >actual</td>\n",
    "  \t<td rowspan=\"2\" ></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  \t<td>true</td>\n",
    "  \t<td>false</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  \t<td rowspan=\"2\" >predicted</td>\n",
    "  \t<td>true</td>\n",
    "  \t<td style='background:green'>60<br>true positives</td>\n",
    "  \t<td style='background:orange'>5<br>false positives</td>\n",
    "  \t<td>65</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  \t<td>false</td>\n",
    "  \t<td style='background:yellow'>10<br>false negatives</td>\n",
    "  \t<td style='background:red'>25<br>true negatives</td>\n",
    "  \t<td>35</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  \t<td colspan=\"2\" ></td>\n",
    "  \t<td>70</td>\n",
    "  \t<td>30</td>\n",
    "  \t<td>100</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "accuracy:<br>\n",
    "percent of right predicted from whole data<br>\n",
    "accuracy = <b style='background:green;'>tp</b>/(<b style='background:green;'>tp</b>+<b style='background:red;'>tn</b>+<b style='background:orange;'>fp</b>+<b style='background:yellow;'>fn</b>) = $\\frac{60+25}{100} = 85 \\%$<br><br>\n",
    "\n",
    "precision:<br>\n",
    "percent of true positives from positive predictions<br>\n",
    "precision = <b style='background:green;'>tp</b>/(<b style='background:green;'>tp</b>+<b style='background:orange;'>fp</b>) = $ \\frac{60}{60 + 5} = 92,3 \\%$<br><br>\n",
    "\n",
    "\n",
    "true positive rate (recall/sensitivity):<br>\n",
    "percent of true positives from all true examples<br>\n",
    "recall/sensitivity = <b style='background:green;'>tp</b>/(<b style='background:green;'>tp</b>+<b style='background:yellow;'>fn</b>) = $\\frac{60}{60+10} = 85,7 \\%$<br><br>\n",
    "\n",
    "f1 score:<br>\n",
    "good error measure, if the data set is unbalanced<br>\n",
    "$f_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall} = 2 \\cdot \\frac{92,3 \\cdot 85,7}{92,3 + 85,7} = 88,9$<br><br>\n",
    "\n",
    "true negative rate (specificity):<br>\n",
    "percent of true negatives from all negative examples<br>\n",
    "specifity = <b style='background:red;'>tn</b>/(<b style='background:red;'>tn</b>+<b style='background:orange;'>fp</b>) = $a$<br><br>\n",
    "\n",
    "false positive rate (fall-out):<br>\n",
    "percent of true negatives from all negative examples<br>\n",
    "fall-out = <b style='background:orange;'>fp</b>/(<b style='background:red;'>tn</b>+<b style='background:orange;'>fp</b>) = $a$<br><br>\n",
    "\n",
    "ROC curve:<br>\n",
    "https://en.wikipedia.org/wiki/Receiver_operating_characteristic#ROC_space\n",
    "true positive rate (sensitivity) plotted against false positive rate (fallout). As a classifier becomes overfitted the true positive rate on the training examples grows as well as the false positive rate.\n",
    "\n",
    "for all possible metrics look at https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian inference\n",
    "\n",
    "Using the bayes theorem we can combine our estimated likelihood with a prior probability distribution, where the likelihood is the actual probability distribution and the prior distribution reflects the bias in our setting (like demographic or ice cream sales in winter) where we apply this likelihood.\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{posterior prob.} = \\frac{\\text{likelihood} \\cdot \\text{prior prob.}}{P(X)} \\\\\n",
    "P(\\Theta|X) = \\frac{P(X|\\Theta) \\cdot P(\\Theta)}{P(X)}\n",
    "\\end{equation}\n",
    "\n",
    "X = observed data<br>\n",
    "$\\Theta$ = model parameters<br>\n",
    "prior prob. = guessed prob. distribution for Parameters <br>\n",
    "likelihood = calculated prob dependent on observed data X<br>\n",
    "posterior prob. = resulting probability distribution of data<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
